{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "readme = open('./All_in_one.md', encoding='utf-8').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "parts = readme.split('\\n##')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Arabic NLP Survey Papers Repository (ASPR) - مستودع الأوراق المسحية في معالجة اللغة العربية (أسبر)\n",
      "\n",
      " General\n",
      " Enabling Tools (Morphology, Diacritization, Grammer, etc.)\n",
      " Text Classification & Mining\n",
      " Named Entity Recognition\n",
      " Quesion Answering\n",
      " Sentiment Analysis\n",
      " Speech\n",
      " Summarization\n",
      " Conversational AI (Chatbots & Dialogue systems)\n",
      " Word Sense Disambiguation (WSD)\n",
      " Dialects\n",
      " Plagiarism Detection Systems\n",
      " Machine Translation\n",
      " Handwriting Recognition & OCR\n",
      " Offensive Language Detection\n",
      " Fake news and Spam\n",
      " Ontology and Resources\n",
      " Information Retrieval\n",
      " Opinion Mining\n",
      " Knowledge Graph \n",
      " Query Expansion\n",
      " Religious Text\n",
      " Others (Steganography, Transfer Learning, Deep learning, etc.) \n"
     ]
    }
   ],
   "source": [
    "for part in parts:\n",
    "    print(part.split('\\n')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2, len(parts)):\n",
    "    part = parts[i]\n",
    "    title = parts[i].split('\\n')[0].strip()\n",
    "    home_link = \"[Table of Contents](README.md)\\n\\n\"\n",
    "    part = home_link + \"#\" + part \n",
    "    filename = str(i-1).zfill(2) + \" \" + title + \".md\"\n",
    "    with open(filename, encoding='utf-8', mode='w') as file_writer:\n",
    "        file_writer.write(part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[General](01 General.md)\n",
      "\n",
      "[Enabling Tools (Morphology, Diacritization, Grammer, etc.)](02 Enabling Tools (Morphology, Diacritization, Grammer, etc.).md)\n",
      "\n",
      "[Text Classification & Mining](03 Text Classification & Mining.md)\n",
      "\n",
      "[Named Entity Recognition](04 Named Entity Recognition.md)\n",
      "\n",
      "[Quesion Answering](05 Quesion Answering.md)\n",
      "\n",
      "[Sentiment Analysis](06 Sentiment Analysis.md)\n",
      "\n",
      "[Speech](07 Speech.md)\n",
      "\n",
      "[Summarization](08 Summarization.md)\n",
      "\n",
      "[Conversational AI (Chatbots & Dialogue systems)](09 Conversational AI (Chatbots & Dialogue systems).md)\n",
      "\n",
      "[Word Sense Disambiguation (WSD)](10 Word Sense Disambiguation (WSD).md)\n",
      "\n",
      "[Dialects](11 Dialects.md)\n",
      "\n",
      "[Plagiarism Detection Systems](12 Plagiarism Detection Systems.md)\n",
      "\n",
      "[Machine Translation](13 Machine Translation.md)\n",
      "\n",
      "[Handwriting Recognition & OCR](14 Handwriting Recognition & OCR.md)\n",
      "\n",
      "[Offensive Language Detection](15 Offensive Language Detection.md)\n",
      "\n",
      "[Fake news and Spam](16 Fake news and Spam.md)\n",
      "\n",
      "[Ontology and Resources](17 Ontology and Resources.md)\n",
      "\n",
      "[Information Retrieval](18 Information Retrieval.md)\n",
      "\n",
      "[Opinion Mining](19 Opinion Mining.md)\n",
      "\n",
      "[Knowledge Graph](20 Knowledge Graph.md)\n",
      "\n",
      "[Query Expansion](21 Query Expansion.md)\n",
      "\n",
      "[Religious Text](22 Religious Text.md)\n",
      "\n",
      "[Others (Steganography, Transfer Learning, Deep learning, etc.)](23 Others (Steganography, Transfer Learning, Deep learning, etc.).md)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(2, len(parts)):\n",
    "    part = parts[i]\n",
    "    title = parts[i].split('\\n')[0].strip()\n",
    "    filename = str(i-1).zfill(2) + \" \" + title + \".md\"\n",
    "    page_link = f\"[{title}]({filename})\\n\"\n",
    "    print(page_link)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "da20024804fad5350fda431ea2db37db985e4ababe34a9d1242fe2d15e5370f4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
